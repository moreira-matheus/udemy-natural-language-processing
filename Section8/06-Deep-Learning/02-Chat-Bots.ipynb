{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../../IMG/Pierian_Data_Logo.png' /></a>\n",
    "___\n",
    "# Question and Answer Chat Bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will be working with the Babi Data Set from Facebook Research.\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Format of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setting up Vocabulary of All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set that holds the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    # In case you don't know what a union of sets is:\n",
    "    # https://www.programiz.com/python-programming/methods/set/union\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for pad_sequences\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default `filters`: '!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'picked': 1,\n",
       " 'garden': 2,\n",
       " 'daniel': 3,\n",
       " 'office': 4,\n",
       " 'left': 5,\n",
       " 'moved': 6,\n",
       " 'sandra': 7,\n",
       " 'to': 8,\n",
       " 'the': 9,\n",
       " 'john': 10,\n",
       " 'there': 11,\n",
       " 'bathroom': 12,\n",
       " 'went': 13,\n",
       " '?': 14,\n",
       " 'in': 15,\n",
       " 'mary': 16,\n",
       " 'kitchen': 17,\n",
       " 'is': 18,\n",
       " 'down': 19,\n",
       " 'milk': 20,\n",
       " 'up': 21,\n",
       " 'dropped': 22,\n",
       " 'bedroom': 23,\n",
       " 'hallway': 24,\n",
       " 'football': 25,\n",
       " 'discarded': 26,\n",
       " 'got': 27,\n",
       " 'grabbed': 28,\n",
       " 'took': 29,\n",
       " 'no': 30,\n",
       " 'travelled': 31,\n",
       " 'back': 32,\n",
       " '.': 33,\n",
       " 'journeyed': 34,\n",
       " 'yes': 35,\n",
       " 'put': 36,\n",
       " 'apple': 37}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalize Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories,Queries,and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
    "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
    "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n",
    "        #\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story,query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  9, 23, 33],\n",
       "       [ 0,  0,  0, ...,  9,  2, 33],\n",
       "       [ 0,  0,  0, ...,  9,  2, 33],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  9, 37, 33],\n",
       "       [ 0,  0,  0, ...,  9,  2, 33],\n",
       "       [ 0,  0,  0, ..., 37, 11, 33]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 10, 15,  9, 17, 14],\n",
       "       [18, 10, 15,  9, 17, 14],\n",
       "       [18, 10, 15,  9,  2, 14],\n",
       "       ...,\n",
       "       [18, 16, 15,  9, 23, 14],\n",
       "       [18,  7, 15,  9,  2, 14],\n",
       "       [18, 16, 15,  9,  2, 14]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 503.,   0.,   0.,\n",
       "         0.,   0., 497.,   0.,   0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for Inputs\n",
    "\n",
    "Recall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building the Networks\n",
    "\n",
    "To understand why we chose this setup, make sure to read the paper we are using:\n",
    "\n",
    "* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "### Input Encoder m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Input gets embedded to a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# (samples, story_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use dot product to compute the match between first input vector seq and the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add this match matrix with the second input vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 6, 220) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce with RNN (LSTM)\n",
    "answer = LSTM(32)(answer)  # (samples, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 500us/step - loss: 0.9067 - acc: 0.4919 - val_loss: 0.6948 - val_acc: 0.4970\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 291us/step - loss: 0.7073 - acc: 0.4946 - val_loss: 0.6933 - val_acc: 0.5030\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 3s 334us/step - loss: 0.6973 - acc: 0.4966 - val_loss: 0.6942 - val_acc: 0.4970\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.6947 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5030\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.6949 - acc: 0.4971 - val_loss: 0.6949 - val_acc: 0.5030\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.6943 - acc: 0.5087 - val_loss: 0.6932 - val_acc: 0.4970\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 0.6948 - acc: 0.4953 - val_loss: 0.6936 - val_acc: 0.5030\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 3s 284us/step - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6943 - val_acc: 0.5030\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 3s 278us/step - loss: 0.6946 - acc: 0.4917 - val_loss: 0.6939 - val_acc: 0.4980\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.6942 - acc: 0.5022 - val_loss: 0.6939 - val_acc: 0.5030\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 3s 305us/step - loss: 0.6936 - acc: 0.5104 - val_loss: 0.6950 - val_acc: 0.4940\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.6917 - acc: 0.5200 - val_loss: 0.6924 - val_acc: 0.5160\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 3s 279us/step - loss: 0.6692 - acc: 0.5852 - val_loss: 0.6478 - val_acc: 0.6510\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.6410 - acc: 0.6312 - val_loss: 0.6370 - val_acc: 0.6430\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.6359 - acc: 0.6407 - val_loss: 0.6176 - val_acc: 0.6650\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 3s 287us/step - loss: 0.6202 - acc: 0.6641 - val_loss: 0.6053 - val_acc: 0.6780\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.6038 - acc: 0.6782 - val_loss: 0.5635 - val_acc: 0.7160\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.5689 - acc: 0.7021 - val_loss: 0.5424 - val_acc: 0.7230\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.5417 - acc: 0.7317 - val_loss: 0.5430 - val_acc: 0.7260\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.5200 - acc: 0.7444 - val_loss: 0.4777 - val_acc: 0.7760\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.4957 - acc: 0.7654 - val_loss: 0.4943 - val_acc: 0.7700\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.4797 - acc: 0.7792 - val_loss: 0.4339 - val_acc: 0.8020\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.4520 - acc: 0.8009 - val_loss: 0.4129 - val_acc: 0.8200\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 3s 293us/step - loss: 0.4285 - acc: 0.8142 - val_loss: 0.4259 - val_acc: 0.8030\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 3s 302us/step - loss: 0.4184 - acc: 0.8193 - val_loss: 0.4948 - val_acc: 0.7740\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 3s 344us/step - loss: 0.4036 - acc: 0.8300 - val_loss: 0.3961 - val_acc: 0.8230\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.3950 - acc: 0.8321 - val_loss: 0.4027 - val_acc: 0.8290\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 4s 360us/step - loss: 0.3837 - acc: 0.8352 - val_loss: 0.3713 - val_acc: 0.8410\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 4s 361us/step - loss: 0.3707 - acc: 0.8397 - val_loss: 0.3635 - val_acc: 0.8310\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.3597 - acc: 0.8457 - val_loss: 0.3548 - val_acc: 0.8330\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.3588 - acc: 0.8466 - val_loss: 0.3512 - val_acc: 0.8400\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.3475 - acc: 0.8487 - val_loss: 0.3528 - val_acc: 0.8390\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 3s 286us/step - loss: 0.3360 - acc: 0.8550 - val_loss: 0.3440 - val_acc: 0.8390\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 3s 306us/step - loss: 0.3374 - acc: 0.8555 - val_loss: 0.3495 - val_acc: 0.8300\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 3s 314us/step - loss: 0.3289 - acc: 0.8545 - val_loss: 0.3396 - val_acc: 0.8340\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.3289 - acc: 0.8546 - val_loss: 0.3366 - val_acc: 0.8460\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.3290 - acc: 0.8586 - val_loss: 0.3488 - val_acc: 0.8390\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.3188 - acc: 0.8600 - val_loss: 0.3483 - val_acc: 0.8420\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.3205 - acc: 0.8608 - val_loss: 0.3440 - val_acc: 0.8340\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.3174 - acc: 0.8602 - val_loss: 0.3418 - val_acc: 0.8400\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.3111 - acc: 0.8651 - val_loss: 0.3401 - val_acc: 0.8370\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.3127 - acc: 0.8640 - val_loss: 0.3617 - val_acc: 0.8450\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.3096 - acc: 0.8644 - val_loss: 0.3463 - val_acc: 0.8450\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.3064 - acc: 0.8655 - val_loss: 0.3558 - val_acc: 0.8360\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 3s 286us/step - loss: 0.3082 - acc: 0.8652 - val_loss: 0.3339 - val_acc: 0.8410\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 4s 355us/step - loss: 0.3025 - acc: 0.8679 - val_loss: 0.3521 - val_acc: 0.8390\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 3s 313us/step - loss: 0.3032 - acc: 0.8665 - val_loss: 0.3400 - val_acc: 0.8350\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 3s 308us/step - loss: 0.3009 - acc: 0.8695 - val_loss: 0.3358 - val_acc: 0.8400\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.3001 - acc: 0.8674 - val_loss: 0.3513 - val_acc: 0.8350\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 3s 279us/step - loss: 0.2997 - acc: 0.8703 - val_loss: 0.3693 - val_acc: 0.8450\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 3s 297us/step - loss: 0.2976 - acc: 0.8708 - val_loss: 0.3409 - val_acc: 0.8390\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2945 - acc: 0.8717 - val_loss: 0.3855 - val_acc: 0.8340\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.2906 - acc: 0.8714 - val_loss: 0.3385 - val_acc: 0.8330\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 3s 292us/step - loss: 0.2914 - acc: 0.8765 - val_loss: 0.3832 - val_acc: 0.8280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 3s 280us/step - loss: 0.2953 - acc: 0.8697 - val_loss: 0.3636 - val_acc: 0.8380\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2902 - acc: 0.8718 - val_loss: 0.3631 - val_acc: 0.8410\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.2894 - acc: 0.8736 - val_loss: 0.3542 - val_acc: 0.8430\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 3s 303us/step - loss: 0.2919 - acc: 0.8736 - val_loss: 0.3498 - val_acc: 0.8400\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 3s 306us/step - loss: 0.2916 - acc: 0.8742 - val_loss: 0.3575 - val_acc: 0.8410\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 3s 303us/step - loss: 0.2914 - acc: 0.8744 - val_loss: 0.3507 - val_acc: 0.8400\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 3s 294us/step - loss: 0.2864 - acc: 0.8750 - val_loss: 0.3823 - val_acc: 0.8370\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.2791 - acc: 0.8751 - val_loss: 0.3762 - val_acc: 0.8360\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.2856 - acc: 0.8759 - val_loss: 0.3555 - val_acc: 0.8380\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.2815 - acc: 0.8767 - val_loss: 0.3739 - val_acc: 0.8280\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 3s 287us/step - loss: 0.2811 - acc: 0.8784 - val_loss: 0.3689 - val_acc: 0.8390\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.2799 - acc: 0.8768 - val_loss: 0.3760 - val_acc: 0.8330\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.2767 - acc: 0.8767 - val_loss: 0.3661 - val_acc: 0.8400\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 3s 284us/step - loss: 0.2788 - acc: 0.8766 - val_loss: 0.3590 - val_acc: 0.8340\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2751 - acc: 0.8798 - val_loss: 0.3668 - val_acc: 0.8280\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.2749 - acc: 0.8776 - val_loss: 0.3976 - val_acc: 0.8240\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2716 - acc: 0.8821 - val_loss: 0.3848 - val_acc: 0.8310\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.2713 - acc: 0.8841 - val_loss: 0.3894 - val_acc: 0.8290\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2735 - acc: 0.8801 - val_loss: 0.3928 - val_acc: 0.8410\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.2679 - acc: 0.8809 - val_loss: 0.3766 - val_acc: 0.8410\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2659 - acc: 0.8838 - val_loss: 0.3867 - val_acc: 0.8280\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 3s 281us/step - loss: 0.2657 - acc: 0.8853 - val_loss: 0.3810 - val_acc: 0.8350\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.2663 - acc: 0.8846 - val_loss: 0.3704 - val_acc: 0.8330\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 0.2660 - acc: 0.8812 - val_loss: 0.3760 - val_acc: 0.8380\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 3s 295us/step - loss: 0.2636 - acc: 0.8850 - val_loss: 0.4136 - val_acc: 0.8330\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 3s 299us/step - loss: 0.2610 - acc: 0.8870 - val_loss: 0.4109 - val_acc: 0.8320\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.2591 - acc: 0.8856 - val_loss: 0.4083 - val_acc: 0.8170\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 0.2618 - acc: 0.8879 - val_loss: 0.4638 - val_acc: 0.8250\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.2531 - acc: 0.8901 - val_loss: 0.4293 - val_acc: 0.8340\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 3s 346us/step - loss: 0.2574 - acc: 0.8875 - val_loss: 0.4145 - val_acc: 0.8310\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.2573 - acc: 0.8854 - val_loss: 0.4266 - val_acc: 0.8210\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 3s 287us/step - loss: 0.2555 - acc: 0.8905 - val_loss: 0.4114 - val_acc: 0.8290\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.2568 - acc: 0.8888 - val_loss: 0.4151 - val_acc: 0.8310\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 3s 294us/step - loss: 0.2542 - acc: 0.8928 - val_loss: 0.4245 - val_acc: 0.8130\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 3s 293us/step - loss: 0.2464 - acc: 0.8906 - val_loss: 0.4090 - val_acc: 0.8320\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 3s 292us/step - loss: 0.2557 - acc: 0.8881 - val_loss: 0.4174 - val_acc: 0.8320\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 3s 302us/step - loss: 0.2460 - acc: 0.8889 - val_loss: 0.4461 - val_acc: 0.8350\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.2441 - acc: 0.8942 - val_loss: 0.4128 - val_acc: 0.8350\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 3s 285us/step - loss: 0.2462 - acc: 0.8886 - val_loss: 0.4096 - val_acc: 0.8320\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 3s 308us/step - loss: 0.2442 - acc: 0.8956 - val_loss: 0.4418 - val_acc: 0.8280\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 3s 302us/step - loss: 0.2472 - acc: 0.8917 - val_loss: 0.4493 - val_acc: 0.8390\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.2435 - acc: 0.8944 - val_loss: 0.4385 - val_acc: 0.8210\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 297us/step - loss: 0.2438 - acc: 0.8953 - val_loss: 0.4543 - val_acc: 0.8190\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 3s 314us/step - loss: 0.2447 - acc: 0.8957 - val_loss: 0.4858 - val_acc: 0.8190\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.2380 - acc: 0.8970 - val_loss: 0.4850 - val_acc: 0.8280\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 3s 299us/step - loss: 0.2328 - acc: 0.8978 - val_loss: 0.4378 - val_acc: 0.8320\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 3s 312us/step - loss: 0.2405 - acc: 0.8948 - val_loss: 0.4524 - val_acc: 0.8360\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 3s 284us/step - loss: 0.2385 - acc: 0.8965 - val_loss: 0.4451 - val_acc: 0.8370\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 3s 289us/step - loss: 0.2308 - acc: 0.9010 - val_loss: 0.4875 - val_acc: 0.8280\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.2327 - acc: 0.8995 - val_loss: 0.5141 - val_acc: 0.8140\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.2328 - acc: 0.8953 - val_loss: 0.4359 - val_acc: 0.8230\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 3s 339us/step - loss: 0.2278 - acc: 0.8991 - val_loss: 0.4518 - val_acc: 0.8290\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 3s 302us/step - loss: 0.2296 - acc: 0.9018 - val_loss: 0.5060 - val_acc: 0.8270\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 3s 277us/step - loss: 0.2355 - acc: 0.8980 - val_loss: 0.4363 - val_acc: 0.8240\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 3s 282us/step - loss: 0.2362 - acc: 0.9002 - val_loss: 0.4667 - val_acc: 0.8370\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 3s 276us/step - loss: 0.2325 - acc: 0.9005 - val_loss: 0.4588 - val_acc: 0.8420\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.2306 - acc: 0.8992 - val_loss: 0.4797 - val_acc: 0.8290\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.2309 - acc: 0.9024 - val_loss: 0.4851 - val_acc: 0.8320\n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 361us/step - loss: 0.2257 - acc: 0.9023 - val_loss: 0.5467 - val_acc: 0.8050\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 3s 347us/step - loss: 0.2269 - acc: 0.9041 - val_loss: 0.5217 - val_acc: 0.8310\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 3s 307us/step - loss: 0.2218 - acc: 0.9021 - val_loss: 0.4660 - val_acc: 0.8290\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 3s 305us/step - loss: 0.2215 - acc: 0.9049 - val_loss: 0.5663 - val_acc: 0.8140\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 3s 294us/step - loss: 0.2256 - acc: 0.9021 - val_loss: 0.4753 - val_acc: 0.8360\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 3s 286us/step - loss: 0.2245 - acc: 0.9032 - val_loss: 0.4674 - val_acc: 0.8340\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 3s 283us/step - loss: 0.2271 - acc: 0.9034 - val_loss: 0.4993 - val_acc: 0.8280\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 3s 286us/step - loss: 0.2224 - acc: 0.8994 - val_loss: 0.5090 - val_acc: 0.8360\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chatbot_120_epochs.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "### Plotting Out Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Given Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'got',\n",
       " 'the',\n",
       " 'milk',\n",
       " 'there',\n",
       " '.',\n",
       " 'John',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " '.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary got the milk there . John moved to the bedroom .\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[0][0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is John in the kitchen ?\n"
     ]
    }
   ],
   "source": [
    "query = ' '.join(word for word in test_data[0][1])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Test Answer from Data is: no\n"
     ]
    }
   ],
   "source": [
    "print(\"True Test Answer from Data is:\",test_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9999987\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Your Own Stories and Questions\n",
    "\n",
    "Remember you can only use words from the existing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'left',\n",
       " 'the',\n",
       " 'kitchen',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'dropped',\n",
       " 'the',\n",
       " 'football',\n",
       " 'in',\n",
       " 'the',\n",
       " 'garden',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the whitespace of the periods\n",
    "my_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\n",
    "my_story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"Is the football in the garden ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'the', 'football', 'in', 'the', 'garden', '?']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(),my_question.split(),'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story,my_ques,my_ans = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  yes\n",
      "Probability of certainty was:  0.9893533\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
